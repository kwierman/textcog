{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwierman/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.1/python-3.6.1-6qs3ck4umkbbao3lu5t3kslx3dr7knxs/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(object):\n",
    "    \n",
    "    def __init__(self, filepath='../data/20news-18828', length=20000):\n",
    "        self.basepath = filepath\n",
    "        self.length=length\n",
    "        self.class_map={}\n",
    "        self.classes = os.listdir(filepath)\n",
    "        for index, value in enumerate(self.classes):\n",
    "            self.class_map[value] = index\n",
    "        self.dataset = None\n",
    "        \n",
    "    def load(self, class_map, dataset):\n",
    "        with open(class_map, 'r') as _file:\n",
    "            self.class_map = copy.copy(json.load(_file))\n",
    "        with open(dataset, 'r') as _file:\n",
    "            self.dataset = copy.copy(json.load(_file))\n",
    "        for cls in self.class_map:\n",
    "            random.shuffle(self.dataset[str(self.class_map[cls])])\n",
    "        \n",
    "    def create_datasets(self):\n",
    "\n",
    "        train = {}\n",
    "        val = {}\n",
    "        test ={}\n",
    "        \n",
    "        for i in self.classes:\n",
    "            train[self.class_map[i]]=[]\n",
    "            val[self.class_map[i]]=[]\n",
    "            test[self.class_map[i]]=[]\n",
    "            for filename in glob.glob(os.path.join(self.basepath, i, '*')):\n",
    "                r = np.random.random_sample()\n",
    "                if r > 0.95:\n",
    "                    test[self.class_map[i]].append(filename)\n",
    "                elif r > 0.9:\n",
    "                    val[self.class_map[i]].append(filename)\n",
    "                else:\n",
    "                    train[self.class_map[i]].append(filename)\n",
    "            random.shuffle(train[self.class_map[i]])\n",
    "            random.shuffle(test[self.class_map[i]])\n",
    "            random.shuffle(val[self.class_map[i]])\n",
    "            \n",
    "        with open('train.json', 'w') as output:\n",
    "            json.dump(train, output)\n",
    "        with open('test.json', 'w') as output:\n",
    "            json.dump(test, output)\n",
    "        with open('val.json', 'w') as output:\n",
    "            json.dump(val, output)                \n",
    "                \n",
    "        with open('class_map.json', 'w') as output:\n",
    "            json.dump(self.class_map, output)\n",
    "\n",
    "    def get_text(self, filename):\n",
    "        output= np.ndarray(shape=(self.length,), dtype=np.integer)\n",
    "        index = 0\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as input_file:\n",
    "            for line in input_file.readlines():\n",
    "                for char in line:\n",
    "                    if index >= self.length:\n",
    "                        break\n",
    "                    output[index] = self.decode_character(char)\n",
    "                    index += 1\n",
    "        return output\n",
    "            \n",
    "    def decode_character(self, char):\n",
    "        try:\n",
    "            return ord(char)\n",
    "        except UnicodeDecodeError:\n",
    "            return 0\n",
    "    \n",
    "    def get_random_filenames(self):\n",
    "        tmp = []\n",
    "        for cls in  self.class_map:\n",
    "            for i in range(2):\n",
    "                try:\n",
    "                    tmp.append( (self.dataset[str(self.class_map[cls])].pop(), self.class_map[cls]))\n",
    "                except IndexError:\n",
    "                    raise StopIteration\n",
    "        random.shuffle(tmp)\n",
    "        return [i[0] for i in tmp], [i[1] for i in tmp]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        x, y = self.get_random_filenames()\n",
    "        tmp_x = []\n",
    "        for i in x:\n",
    "            encoding = self.get_text(i)\n",
    "            tmp_x.append(encoding)\n",
    "        x = tmp_x    \n",
    "        tmp_x = np.zeros(shape=(len(x), self.length))\n",
    "        for index, arr in enumerate(x):\n",
    "            tmp_x[index][:len(arr)] = arr\n",
    "        # So, now we're going to have to create the Y matrix\n",
    "        tmp_y = np.zeros(shape=(len(y),20))\n",
    "        for index, value in enumerate(y):\n",
    "                tmp_y[index, value] =1\n",
    "        return tmp_x, tmp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, batch_size=40):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = tf.placeholder(tf.float32, [None, 20000], name=\"X\")\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 20], name=\"Y\")\n",
    "\n",
    "        X = tf.reshape(self.X, shape=[self.batch_size, 20000, 1])\n",
    "        self.a1 = self.create_conv_layer(X, 5, 3, 128, 1)\n",
    "        width = int((20000-5)/3+1)\n",
    "        width = int((width-5)/3+1)\n",
    "        self.a2 = self.create_conv_layer(self.a1, 3, 2, 256, 128, width)\n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        self.a3 = self.create_conv_layer(self.a2, 3, 2, 256, 256, width)      \n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        self.flatten =  tf.reshape(self.a3, [-1, width*256])\n",
    "        \n",
    "        self.dense1 = tf.layers.dense(inputs=self.flatten, units=int(width/2),\n",
    "                                      activation=tf.nn.relu, name='dense1')\n",
    "        self.do1 = tf.nn.dropout(self.dense1, 0.5)\n",
    "        self.dense2 = tf.layers.dense(inputs=self.do1, units=int(width/4),\n",
    "                                      activation=tf.nn.relu, name='dense2') \n",
    "        self.do2 = tf.nn.dropout(self.dense2, 0.5)\n",
    "        self.dense3 = tf.layers.dense(inputs=self.do2, units=20,\n",
    "                                      activation=tf.nn.sigmoid, name='logits')\n",
    "        \n",
    "        self.softmax = tf.nn.softmax(self.dense3, name=\"softmax_tensor\")\n",
    "        self.predictions = tf.argmax(self.softmax, 1, name=\"predictions\")\n",
    "        \n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.dense3)\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        correct_predictions = tf.equal(self.predictions, tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        \n",
    "    def create_conv_layer(self, input_tensor, kernel_shape=3, stride=1, \n",
    "                          n_filters=64, in_channels=1, input_width=20000):\n",
    "        with tf.name_scope('convlayer_{}_{}_{}_{}'.format(kernel_shape, stride, n_filters, in_channels)):\n",
    "            filter_shape = [kernel_shape, in_channels, n_filters]\n",
    "            W = tf.Variable(tf.random_uniform(filter_shape), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[n_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv1d(input_tensor, W, stride=stride, padding=\"VALID\", name=\"conv\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "            expected_shape = int((input_width-kernel_shape)/stride+1)\n",
    "            pooled = tf.nn.max_pool(tf.reshape(h, [-1, expected_shape,n_filters, 1]),\n",
    "                                    ksize=[1, kernel_shape, 1, 1],\n",
    "                                    strides=[1, stride, 1, 1],\n",
    "                                    padding='VALID',\n",
    "                                    name=\"pool\")\n",
    "            expected_shape = int((expected_shape-kernel_shape)/stride+1)\n",
    "            return tf.reshape(pooled,  [-1, expected_shape,n_filters])\n",
    "\n",
    "    def train(self, sess):\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(5e-2)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train_gen = TextDataSet()\n",
    "            train_gen.load('class_map.json', 'train.json')\n",
    "\n",
    "            test_gen = TextDataSet()\n",
    "            test_gen.load('class_map.json', 'test.json')\n",
    "\n",
    "            if epoch >0:\n",
    "                save_path = saver.save(sess, checkpoint_prefix)\n",
    "                print(\"Saved model to {}\".format(save_path))\n",
    "            step=0\n",
    "            for x_batch, y_batch in train_gen:\n",
    "\n",
    "                feed_dict = {\n",
    "                  self.X: x_batch,\n",
    "                  self.Y: y_batch\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, self.loss, self.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "                step+=1\n",
    "\n",
    "                try:\n",
    "                    x_batch, ybatch = next(test_gen)\n",
    "                except StopIteration:\n",
    "                    test_gen = TextDataSet()\n",
    "                    test_gen.load('class_map.json', 'test.json')\n",
    "                    x_batch, ybatch = next(test_gen)\n",
    "\n",
    "                feed_dict = {\n",
    "                  self.X: x_batch,\n",
    "                  self.Y: y_batch\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, self.loss, self.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                dev_summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028\n",
      "\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n",
      "Saved model to /home/kwierman/Desktop/textrecog/notebook/runs/1516750028/checkpoints/model\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN()\n",
    "        cnn.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
