{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwierman/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.1/python-3.6.1-6qs3ck4umkbbao3lu5t3kslx3dr7knxs/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(object):\n",
    "    \n",
    "    def __init__(self, filepath='../data/20news-18828', length=20000):\n",
    "        self.basepath = filepath\n",
    "        self.length=length\n",
    "        self.class_map={}\n",
    "        self.classes = os.listdir(filepath)\n",
    "        for index, value in enumerate(self.classes):\n",
    "            self.class_map[value] = index\n",
    "        self.dataset = None\n",
    "        \n",
    "    def load(self, class_map, dataset):\n",
    "        with open(class_map, 'r') as _file:\n",
    "            self.class_map = copy.copy(json.load(_file))\n",
    "        with open(dataset, 'r') as _file:\n",
    "            self.dataset = copy.copy(json.load(_file))\n",
    "        for cls in self.class_map:\n",
    "            random.shuffle(self.dataset[str(self.class_map[cls])])\n",
    "        \n",
    "    def create_datasets(self):\n",
    "\n",
    "        train = {}\n",
    "        val = {}\n",
    "        test ={}\n",
    "        \n",
    "        for i in self.classes:\n",
    "            train[self.class_map[i]]=[]\n",
    "            val[self.class_map[i]]=[]\n",
    "            test[self.class_map[i]]=[]\n",
    "            for filename in glob.glob(os.path.join(self.basepath, i, '*')):\n",
    "                r = np.random.random_sample()\n",
    "                if r > 0.95:\n",
    "                    test[self.class_map[i]].append(filename)\n",
    "                elif r > 0.9:\n",
    "                    val[self.class_map[i]].append(filename)\n",
    "                else:\n",
    "                    train[self.class_map[i]].append(filename)\n",
    "            random.shuffle(train[self.class_map[i]])\n",
    "            random.shuffle(test[self.class_map[i]])\n",
    "            random.shuffle(val[self.class_map[i]])\n",
    "            \n",
    "        with open('train.json', 'w') as output:\n",
    "            json.dump(train, output)\n",
    "        with open('test.json', 'w') as output:\n",
    "            json.dump(test, output)\n",
    "        with open('val.json', 'w') as output:\n",
    "            json.dump(val, output)                \n",
    "                \n",
    "        with open('class_map.json', 'w') as output:\n",
    "            json.dump(self.class_map, output)\n",
    "\n",
    "    def get_text(self, filename):\n",
    "        output= np.ndarray(shape=(self.length,), dtype=np.integer)\n",
    "        index = 0\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as input_file:\n",
    "            for line in input_file.readlines():\n",
    "                for char in line:\n",
    "                    if index >= self.length:\n",
    "                        break\n",
    "                    output[index] = self.decode_character(char)\n",
    "                    index += 1\n",
    "        return output\n",
    "            \n",
    "    def decode_character(self, char):\n",
    "        try:\n",
    "            return ord(char)\n",
    "        except UnicodeDecodeError:\n",
    "            return 0\n",
    "    \n",
    "    def get_random_filenames(self):\n",
    "        tmp = []\n",
    "        for cls in  self.class_map:\n",
    "            try:\n",
    "                tmp.append( (self.dataset[str(self.class_map[cls])].pop(), self.class_map[cls]))\n",
    "            except IndexError:\n",
    "                raise StopIteration\n",
    "        random.shuffle(tmp)\n",
    "        return [i[0] for i in tmp], [i[1] for i in tmp]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        x, y = self.get_random_filenames()\n",
    "        tmp_x = []\n",
    "        for i in x:\n",
    "            encoding = self.get_text(i)\n",
    "            tmp_x.append(encoding)\n",
    "        x = tmp_x    \n",
    "        tmp_x = np.zeros(shape=(len(self.class_map), self.length))\n",
    "        for index, arr in enumerate(x):\n",
    "            tmp_x[index][:len(arr)] = arr\n",
    "        # So, now we're going to have to create the Y matrix\n",
    "        tmp_y = np.zeros(shape=(20,20))\n",
    "        for index, value in enumerate(y):\n",
    "                tmp_y[index, value] =1\n",
    "        return tmp_x, tmp_y\n",
    "    \n",
    "def get_train_gen():\n",
    "    train_gen = TextDataSet()\n",
    "    train_gen.load('class_map.json', 'train.json')\n",
    "    return train_gen\n",
    "\n",
    "def get_test_gen():\n",
    "    test_gen = TextDataSet()\n",
    "    test_gen.load('class_map.json', 'test.json')\n",
    "    return test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, input_width=20000, n_classes=20):\n",
    "        self.input_width = input_width\n",
    "        self.n_classes = n_classes\n",
    "        self.softmax = None\n",
    "        self.predictions = None\n",
    "    \n",
    "    def compile(self, train=True):\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.input_width], name=\"X\")\n",
    "        self.Y = tf.placeholder(tf.float32, [None, self.n_classes], name=\"Y\")\n",
    "\n",
    "        X = tf.reshape(self.X, shape=[-1, self.input_width, 1])\n",
    "        X = self.create_conv_layer(X, 3, 2, 128, 1)\n",
    "        width = int((self.input_width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        X = self.create_conv_layer(X, 3, 2, 256, 128, width)\n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        X = self.create_conv_layer(X, 3, 2, 512, 256, width)      \n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        X =  tf.reshape(X, [-1, width*512])\n",
    "        \n",
    "        X = tf.layers.dense(inputs=X, units=int(width/2),\n",
    "                                      activation=tf.nn.relu, name='dense1')\n",
    "        if train:\n",
    "            X = tf.nn.dropout(X, 0.5)\n",
    "        X = tf.layers.dense(inputs=X, units=int(width/4),\n",
    "                            activation=tf.nn.relu, name='dense2')\n",
    "        if train:\n",
    "            self.X = tf.nn.dropout(X, 0.5)\n",
    "        self.X = tf.layers.dense(inputs=X, units=20,\n",
    "                                activation=tf.nn.relu, name='logits')\n",
    "        \n",
    "        self.softmax = tf.nn.softmax(X, name=\"softmax_tensor\")\n",
    "        self.predictions = tf.argmax(X, 1, name=\"predictions\")\n",
    "        \n",
    "    def create_conv_layer(self, input_tensor, kernel_shape=3, stride=1, \n",
    "                          n_filters=64, in_channels=1, input_width=20000):\n",
    "        with tf.name_scope('convlayer_{}_{}_{}_{}'.format(kernel_shape, stride, n_filters, in_channels)):\n",
    "            filter_shape = [kernel_shape, in_channels, n_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[n_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv1d(input_tensor, W, stride=stride, padding=\"VALID\", name=\"conv\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "            expected_shape = int((input_width-kernel_shape)/stride+1)\n",
    "            pooled = tf.nn.max_pool(tf.reshape(h, [-1, expected_shape,n_filters, 1]),\n",
    "                                    ksize=[1, kernel_shape, 1, 1],\n",
    "                                    strides=[1, stride, 1, 1],\n",
    "                                    padding='VALID',\n",
    "                                    name=\"pool\")\n",
    "            expected_shape = int((expected_shape-kernel_shape)/stride+1)\n",
    "            return tf.reshape(pooled,  [-1, expected_shape,n_filters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "    \n",
    "    def train(self, model, n_epochs, train_genf, test_genf):\n",
    "        with tf.Graph().as_default():\n",
    "            sess = tf.Session()\n",
    "            with sess.as_default():\n",
    "                model.compile(train=True)\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(labels=model.Y, logits=model.softmax)\n",
    "                self.loss = tf.reduce_mean(losses)\n",
    "                correct_predictions = tf.equal(model.predictions, tf.argmax(model.Y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")                \n",
    "\n",
    "                global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "                optimizer = tf.train.AdamOptimizer(2e-1)\n",
    "                grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "                train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "                # Output directory for models and summaries\n",
    "                timestamp = str(int(time.time()))\n",
    "                out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "                print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "                # Summaries for loss and accuracy\n",
    "                loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "                acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "\n",
    "                # Train Summaries\n",
    "                train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "                train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "                train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "                # Dev summaries\n",
    "                dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "                dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "                dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "                # Checkpointing\n",
    "                checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "                checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "                # Tensorflow assumes this directory already exists so we need to create it\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir)\n",
    "                saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "                tf.global_variables_initializer().run()\n",
    "\n",
    "                for epoch in range(100):\n",
    "                    train_gen = train_genf()\n",
    "                    test_gen = test_genf()\n",
    "\n",
    "                    step=0\n",
    "                    for x_batch, y_batch in train_gen:\n",
    "                        feed_dict = {\n",
    "                          model.X: x_batch,\n",
    "                          model.Y: y_batch\n",
    "                        }\n",
    "                        _, step, summaries, loss, accuracy = sess.run(\n",
    "                            [train_op, global_step, train_summary_op, self.loss, self.accuracy],\n",
    "                            feed_dict)\n",
    "                        time_str = datetime.datetime.now().isoformat()\n",
    "                        train_summary_writer.add_summary(summaries, step)\n",
    "                        step+=1\n",
    "\n",
    "                        try:\n",
    "                            x_batch, ybatch = next(test_gen)\n",
    "                        except StopIteration:\n",
    "                            test_gen = test_genf()\n",
    "                            x_batch, ybatch = next(test_gen)\n",
    "                        feed_dict = {\n",
    "                          model.X: x_batch,\n",
    "                          model.Y: y_batch\n",
    "                        }\n",
    "                        step, summaries, loss, accuracy = sess.run(\n",
    "                            [global_step, dev_summary_op, model.loss, model.accuracy],\n",
    "                            feed_dict)\n",
    "                        time_str = datetime.datetime.now().isoformat()\n",
    "                        dev_summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kwierman/Desktop/textrecog/notebook/runs/1516576104\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_genf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a5926044ce8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_train_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_test_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-971b05e5e22b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, n_epochs, train_genf, val_genf)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mtrain_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_genf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                     \u001b[0mtest_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_genf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_genf' is not defined"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "trainer = Trainer()\n",
    "trainer.train(model, 10, get_train_gen, get_test_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
