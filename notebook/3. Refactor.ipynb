{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwierman/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.1/python-3.6.1-6qs3ck4umkbbao3lu5t3kslx3dr7knxs/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(object):\n",
    "    \n",
    "    def __init__(self, filepath='../data/20news-18828', length=20000):\n",
    "        self.basepath = filepath\n",
    "        self.length=length\n",
    "        self.class_map={}\n",
    "        self.classes = os.listdir(filepath)\n",
    "        for index, value in enumerate(self.classes):\n",
    "            self.class_map[value] = index\n",
    "        self.dataset = None\n",
    "        \n",
    "    def load(self, class_map, dataset):\n",
    "        with open(class_map, 'r') as _file:\n",
    "            self.class_map = copy.copy(json.load(_file))\n",
    "        with open(dataset, 'r') as _file:\n",
    "            self.dataset = copy.copy(json.load(_file))\n",
    "        for cls in self.class_map:\n",
    "            random.shuffle(self.dataset[str(self.class_map[cls])])\n",
    "        \n",
    "    def create_datasets(self):\n",
    "\n",
    "        train = {}\n",
    "        val = {}\n",
    "        test ={}\n",
    "        \n",
    "        for i in self.classes:\n",
    "            train[self.class_map[i]]=[]\n",
    "            val[self.class_map[i]]=[]\n",
    "            test[self.class_map[i]]=[]\n",
    "            for filename in glob.glob(os.path.join(self.basepath, i, '*')):\n",
    "                r = np.random.random_sample()\n",
    "                if r > 0.95:\n",
    "                    test[self.class_map[i]].append(filename)\n",
    "                elif r > 0.9:\n",
    "                    val[self.class_map[i]].append(filename)\n",
    "                else:\n",
    "                    train[self.class_map[i]].append(filename)\n",
    "            random.shuffle(train[self.class_map[i]])\n",
    "            random.shuffle(test[self.class_map[i]])\n",
    "            random.shuffle(val[self.class_map[i]])\n",
    "            \n",
    "        with open('train.json', 'w') as output:\n",
    "            json.dump(train, output)\n",
    "        with open('test.json', 'w') as output:\n",
    "            json.dump(test, output)\n",
    "        with open('val.json', 'w') as output:\n",
    "            json.dump(val, output)                \n",
    "                \n",
    "        with open('class_map.json', 'w') as output:\n",
    "            json.dump(self.class_map, output)\n",
    "\n",
    "    def get_text(self, filename):\n",
    "        output= np.ndarray(shape=(self.length,), dtype=np.integer)\n",
    "        index = 0\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as input_file:\n",
    "            for line in input_file.readlines():\n",
    "                for char in line:\n",
    "                    if index >= self.length:\n",
    "                        break\n",
    "                    output[index] = self.decode_character(char)\n",
    "                    index += 1\n",
    "        return output\n",
    "            \n",
    "    def decode_character(self, char):\n",
    "        try:\n",
    "            return ord(char)\n",
    "        except UnicodeDecodeError:\n",
    "            return 0\n",
    "    \n",
    "    def get_random_filenames(self):\n",
    "        tmp = []\n",
    "        for cls in  self.class_map:\n",
    "            try:\n",
    "                tmp.append( (self.dataset[str(self.class_map[cls])].pop(), self.class_map[cls]))\n",
    "            except IndexError:\n",
    "                raise StopIteration\n",
    "        random.shuffle(tmp)\n",
    "        return [i[0] for i in tmp], [i[1] for i in tmp]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        x, y = self.get_random_filenames()\n",
    "        tmp_x = []\n",
    "        for i in x:\n",
    "            encoding = self.get_text(i)\n",
    "            tmp_x.append(encoding)\n",
    "        x = tmp_x    \n",
    "        tmp_x = np.zeros(shape=(len(self.class_map), self.length))\n",
    "        for index, arr in enumerate(x):\n",
    "            tmp_x[index][:len(arr)] = arr\n",
    "        # So, now we're going to have to create the Y matrix\n",
    "        tmp_y = np.zeros(shape=(20,20))\n",
    "        for index, value in enumerate(y):\n",
    "                tmp_y[index, value] =1\n",
    "        return tmp_x, tmp_y\n",
    "    \n",
    "def get_train_gen():\n",
    "    train_gen = TextDataSet()\n",
    "    train_gen.load('class_map.json', 'train.json')\n",
    "    return train_gen\n",
    "\n",
    "def get_test_gen():\n",
    "    test_gen = TextDataSet()\n",
    "    test_gen.load('class_map.json', 'test.json')\n",
    "    return test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, input_width=20000, n_classes=20):\n",
    "        self.input_width = input_width\n",
    "        self.n_classes = n_classes\n",
    "        self.softmax = None\n",
    "        self.predictions = None\n",
    "    \n",
    "    def compile(self, train=True):\n",
    "        X = tf.placeholder(tf.float32, [None, self.input_width], name=\"X\")\n",
    "        Y = tf.placeholder(tf.float32, [None, self.n_classes], name=\"Y\")\n",
    "\n",
    "        X = tf.reshape(self.X, shape=[-1, self.input_width, 1])\n",
    "        X = self.create_conv_layer(X, 3, 2, 128, 1)\n",
    "        width = int((self.input_width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        X = self.create_conv_layer(X, 3, 2, 256, 128, width)\n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        X = self.create_conv_layer(X, 3, 2, 512, 256, width)      \n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        X =  tf.reshape(X, [-1, width*512])\n",
    "        \n",
    "        X = tf.layers.dense(inputs=X, units=int(width/2),\n",
    "                                      activation=tf.nn.relu, name='dense1')\n",
    "        if train:\n",
    "            X = tf.nn.dropout(X, 0.5)\n",
    "        X = tf.layers.dense(inputs=X, units=int(width/4),\n",
    "                            activation=tf.nn.relu, name='dense2')\n",
    "        if train:\n",
    "            self.X = tf.nn.dropout(X, 0.5)\n",
    "        self.X = tf.layers.dense(inputs=X, units=20,\n",
    "                                activation=tf.nn.relu, name='logits')\n",
    "        \n",
    "        self.softmax = tf.nn.softmax(X, name=\"softmax_tensor\")\n",
    "        self.predictions = tf.argmax(X, 1, name=\"predictions\")\n",
    "        \n",
    "    def create_conv_layer(self, input_tensor, kernel_shape=3, stride=1, \n",
    "                          n_filters=64, in_channels=1, input_width=20000):\n",
    "        with tf.name_scope('convlayer_{}_{}_{}_{}'.format(kernel_shape, stride, n_filters, in_channels)):\n",
    "            filter_shape = [kernel_shape, in_channels, n_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[n_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv1d(input_tensor, W, stride=stride, padding=\"VALID\", name=\"conv\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "            expected_shape = int((input_width-kernel_shape)/stride+1)\n",
    "            pooled = tf.nn.max_pool(tf.reshape(h, [-1, expected_shape,n_filters, 1]),\n",
    "                                    ksize=[1, kernel_shape, 1, 1],\n",
    "                                    strides=[1, stride, 1, 1],\n",
    "                                    padding='VALID',\n",
    "                                    name=\"pool\")\n",
    "            expected_shape = int((expected_shape-kernel_shape)/stride+1)\n",
    "            return tf.reshape(pooled,  [-1, expected_shape,n_filters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "    \n",
    "    def train(self, model, n_epochs):\n",
    "        with tf.Graph().as_default():\n",
    "            sess = tf.Session()\n",
    "            with sess.as_default():\n",
    "                model.compile(train=True)\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(labels=model.Y, logits=model.softmax)\n",
    "                self.loss = tf.reduce_mean(losses)\n",
    "                correct_predictions = tf.equal(model.predictions, tf.argmax(model.Y, 1))\n",
    "                self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")                \n",
    "\n",
    "                \n",
    "                \n",
    "                global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(2e-1)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    def train(self, sess):\n",
    "\n",
    "\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(100)):\n",
    "            train_gen = TextDataSet()\n",
    "            train_gen.load('class_map.json', 'train.json')\n",
    "\n",
    "            test_gen = TextDataSet()\n",
    "            test_gen.load('class_map.json', 'train.json')\n",
    "\n",
    "            step=0\n",
    "            for x_batch, y_batch in tqdm.tqdm(train_gen):\n",
    "\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  self.X: x_batch,\n",
    "                  self.Y: y_batch\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, self.loss, self.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "                step+=1\n",
    "\n",
    "                try:\n",
    "                    x_batch, ybatch = next(test_gen)\n",
    "                except StopIteration:\n",
    "                    test_gen = TextDataSet()\n",
    "                    test_gen.load('class_map.json', 'train.json')\n",
    "                    x_batch, ybatch = next(test_gen)\n",
    "\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  self.X: x_batch,\n",
    "                  self.Y: y_batch\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, self.loss, self.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                dev_summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kwierman/Desktop/textrecog/notebook/runs/1516566887\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:08,  8.05s/it]\u001b[A\n",
      "2it [00:16,  8.48s/it]\u001b[A\n",
      "3it [00:27,  9.07s/it]\u001b[A\n",
      "4it [00:38,  9.58s/it]\u001b[A\n",
      "5it [00:49,  9.89s/it]\u001b[A\n",
      "6it [01:00, 10.14s/it]\u001b[A\n",
      "7it [01:12, 10.32s/it]\u001b[A\n",
      "8it [01:22, 10.37s/it]\u001b[A\n",
      "9it [01:33, 10.36s/it]\u001b[A\n",
      "10it [01:43, 10.39s/it]\u001b[A\n",
      "11it [01:54, 10.39s/it]\u001b[A\n",
      "12it [02:04, 10.41s/it]\u001b[A\n",
      "13it [02:15, 10.42s/it]\u001b[A\n",
      "14it [02:26, 10.49s/it]\u001b[A\n",
      "15it [02:37, 10.50s/it]\u001b[A\n",
      "16it [02:48, 10.52s/it]\u001b[A\n",
      "17it [02:59, 10.54s/it]\u001b[A\n",
      "18it [03:09, 10.55s/it]\u001b[A\n",
      "19it [03:20, 10.56s/it]\u001b[A\n",
      "20it [03:31, 10.57s/it]\u001b[A\n",
      "21it [03:42, 10.58s/it]\u001b[A\n",
      "22it [03:53, 10.60s/it]\u001b[A\n",
      "23it [04:04, 10.64s/it]\u001b[A\n",
      "24it [04:15, 10.66s/it]\u001b[A\n",
      "25it [04:26, 10.66s/it]\u001b[A\n",
      "26it [04:37, 10.68s/it]\u001b[A\n",
      "27it [04:48, 10.70s/it]\u001b[A\n",
      "28it [05:00, 10.72s/it]\u001b[A\n",
      "29it [05:11, 10.74s/it]\u001b[A\n",
      "30it [05:22, 10.76s/it]\u001b[A\n",
      "31it [05:33, 10.76s/it]\u001b[A\n",
      "32it [05:45, 10.79s/it]\u001b[A\n",
      "33it [05:56, 10.79s/it]\u001b[A\n",
      "34it [06:08, 10.83s/it]\u001b[A\n",
      "35it [06:19, 10.83s/it]\u001b[A\n",
      "36it [06:30, 10.84s/it]\u001b[A\n",
      "37it [06:40, 10.84s/it]\u001b[A\n",
      "38it [06:52, 10.86s/it]\u001b[A\n",
      "39it [07:04, 10.88s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN()\n",
    "        cnn.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
