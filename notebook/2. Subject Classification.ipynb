{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwierman/spack/opt/spack/linux-ubuntu16.04-x86_64/gcc-5.4.1/python-3.6.1-6qs3ck4umkbbao3lu5t3kslx3dr7knxs/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet(object):\n",
    "    \n",
    "    def __init__(self, filepath='../data/20news-18828', length=20000):\n",
    "        self.basepath = filepath\n",
    "        self.length=length\n",
    "        self.class_map={}\n",
    "        self.classes = os.listdir(filepath)\n",
    "        for index, value in enumerate(self.classes):\n",
    "            self.class_map[value] = index\n",
    "        self.dataset = None\n",
    "        \n",
    "    def load(self, class_map, dataset):\n",
    "        with open(class_map, 'r') as _file:\n",
    "            self.class_map = copy.copy(json.load(_file))\n",
    "        with open(dataset, 'r') as _file:\n",
    "            self.dataset = copy.copy(json.load(_file))\n",
    "        for cls in self.class_map:\n",
    "            random.shuffle(self.dataset[str(self.class_map[cls])])\n",
    "        \n",
    "    def create_datasets(self):\n",
    "\n",
    "        train = {}\n",
    "        val = {}\n",
    "        test ={}\n",
    "        \n",
    "        for i in self.classes:\n",
    "            train[self.class_map[i]]=[]\n",
    "            val[self.class_map[i]]=[]\n",
    "            test[self.class_map[i]]=[]\n",
    "            for filename in glob.glob(os.path.join(self.basepath, i, '*')):\n",
    "                r = np.random.random_sample()\n",
    "                if r > 0.95:\n",
    "                    test[self.class_map[i]].append(filename)\n",
    "                elif r > 0.9:\n",
    "                    val[self.class_map[i]].append(filename)\n",
    "                else:\n",
    "                    train[self.class_map[i]].append(filename)\n",
    "            random.shuffle(train[self.class_map[i]])\n",
    "            random.shuffle(test[self.class_map[i]])\n",
    "            random.shuffle(val[self.class_map[i]])\n",
    "            \n",
    "        with open('train.json', 'w') as output:\n",
    "            json.dump(train, output)\n",
    "        with open('test.json', 'w') as output:\n",
    "            json.dump(test, output)\n",
    "        with open('val.json', 'w') as output:\n",
    "            json.dump(val, output)                \n",
    "                \n",
    "        with open('class_map.json', 'w') as output:\n",
    "            json.dump(self.class_map, output)\n",
    "\n",
    "    def get_text(self, filename):\n",
    "        output= np.ndarray(shape=(self.length,), dtype=np.integer)\n",
    "        index = 0\n",
    "        with open(filename, 'r', encoding='utf-8', errors='ignore') as input_file:\n",
    "            for line in input_file.readlines():\n",
    "                for char in line:\n",
    "                    if index >= self.length:\n",
    "                        break\n",
    "                    output[index] = self.decode_character(char)\n",
    "                    index += 1\n",
    "        return output\n",
    "            \n",
    "    def decode_character(self, char):\n",
    "        try:\n",
    "            return ord(char)\n",
    "        except UnicodeDecodeError:\n",
    "            return 0\n",
    "    \n",
    "    def get_random_filenames(self):\n",
    "        tmp = []\n",
    "        for cls in  self.class_map:\n",
    "            try:\n",
    "                tmp.append( (self.dataset[str(self.class_map[cls])].pop(), self.class_map[cls]))\n",
    "            except IndexError:\n",
    "                raise StopIteration\n",
    "        random.shuffle(tmp)\n",
    "        return [i[0] for i in tmp], [i[1] for i in tmp]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return self.next()\n",
    "    \n",
    "    def next(self):\n",
    "        x, y = self.get_random_filenames()\n",
    "        tmp_x = []\n",
    "        for i in x:\n",
    "            encoding = self.get_text(i)\n",
    "            tmp_x.append(encoding)\n",
    "        x = tmp_x    \n",
    "        tmp_x = np.zeros(shape=(len(self.class_map), self.length))\n",
    "        for index, arr in enumerate(x):\n",
    "            tmp_x[index][:len(arr)] = arr\n",
    "        # So, now we're going to have to create the Y matrix\n",
    "        tmp_y = np.zeros(shape=(20,20))\n",
    "        for index, value in enumerate(y):\n",
    "                tmp_y[index, value] =1\n",
    "        return tmp_x, tmp_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \n",
    "    def __init__(self, batch_size=20):\n",
    "        self.batch_size = batch_size\n",
    "        self.X = tf.placeholder(tf.float32, [None, 20000], name=\"X\")\n",
    "        self.Y = tf.placeholder(tf.float32, [None, 20], name=\"Y\")\n",
    "\n",
    "        X = tf.reshape(self.X, shape=[self.batch_size, 20000, 1])\n",
    "        self.a1 = self.create_conv_layer(X, 3, 2, 128, 1)\n",
    "        width = int((20000-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        self.a2 = self.create_conv_layer(self.a1, 3, 2, 256, 128, width)\n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        self.a3 = self.create_conv_layer(self.a2, 3, 2, 512, 256, width)      \n",
    "        width = int((width-3)/2+1)\n",
    "        width = int((width-3)/2+1)\n",
    "        self.flatten =  tf.reshape(self.a3, [-1, width*512])\n",
    "        \n",
    "        self.dense1 = tf.layers.dense(inputs=self.flatten, units=int(width/2),\n",
    "                                      activation=tf.nn.relu, name='dense1')\n",
    "        self.do1 = tf.nn.dropout(self.dense1, 0.5)\n",
    "        self.dense2 = tf.layers.dense(inputs=self.do1, units=int(width/4),\n",
    "                                      activation=tf.nn.relu, name='dense2') \n",
    "        self.do2 = tf.nn.dropout(self.dense2, 0.5)\n",
    "        self.dense3 = tf.layers.dense(inputs=self.do2, units=20,\n",
    "                                      activation=tf.nn.relu, name='logits')\n",
    "        \n",
    "        self.softmax = tf.nn.softmax(self.dense3, name=\"softmax_tensor\")\n",
    "        self.predictions = tf.argmax(self.softmax, 1, name=\"predictions\")\n",
    "        \n",
    "        losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.Y, logits=self.softmax)\n",
    "        self.loss = tf.reduce_mean(losses)\n",
    "        \n",
    "        correct_predictions = tf.equal(self.predictions, tf.argmax(self.Y, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n",
    "        \n",
    "    def create_conv_layer(self, input_tensor, kernel_shape=3, stride=1, \n",
    "                          n_filters=64, in_channels=1, input_width=20000):\n",
    "        with tf.name_scope('convlayer_{}_{}_{}_{}'.format(kernel_shape, stride, n_filters, in_channels)):\n",
    "            filter_shape = [kernel_shape, in_channels, n_filters]\n",
    "            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[n_filters]), name=\"b\")\n",
    "            conv = tf.nn.conv1d(input_tensor, W, stride=stride, padding=\"VALID\", name=\"conv\")\n",
    "            h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "            \n",
    "            expected_shape = int((input_width-kernel_shape)/stride+1)\n",
    "            pooled = tf.nn.max_pool(tf.reshape(h, [-1, expected_shape,n_filters, 1]),\n",
    "                                    ksize=[1, kernel_shape, 1, 1],\n",
    "                                    strides=[1, stride, 1, 1],\n",
    "                                    padding='VALID',\n",
    "                                    name=\"pool\")\n",
    "            expected_shape = int((expected_shape-kernel_shape)/stride+1)\n",
    "            return tf.reshape(pooled,  [-1, expected_shape,n_filters])\n",
    "\n",
    "    def train(self, sess):\n",
    "\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(2e-1)\n",
    "        grads_and_vars = optimizer.compute_gradients(self.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", self.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpointing\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "        # Tensorflow assumes this directory already exists so we need to create it\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for epoch in tqdm.tqdm(range(100)):\n",
    "            train_gen = TextDataSet()\n",
    "            train_gen.load('class_map.json', 'train.json')\n",
    "\n",
    "            test_gen = TextDataSet()\n",
    "            test_gen.load('class_map.json', 'train.json')\n",
    "\n",
    "            if epoch >0:\n",
    "                save_path = saver.save(sess, \"checkpoint_prefix\")\n",
    "                print(\"Saved model to {}\".format(save_path))\n",
    "            step=0\n",
    "            for x_batch, y_batch in tqdm.tqdm(train_gen):\n",
    "\n",
    "                \"\"\"\n",
    "                A single training step\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  self.X: x_batch,\n",
    "                  self.Y: y_batch\n",
    "                }\n",
    "                _, step, summaries, loss, accuracy = sess.run(\n",
    "                    [train_op, global_step, train_summary_op, self.loss, self.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                train_summary_writer.add_summary(summaries, step)\n",
    "                step+=1\n",
    "\n",
    "                try:\n",
    "                    x_batch, ybatch = next(test_gen)\n",
    "                except StopIteration:\n",
    "                    test_gen = TextDataSet()\n",
    "                    test_gen.load('class_map.json', 'train.json')\n",
    "                    x_batch, ybatch = next(test_gen)\n",
    "\n",
    "                \"\"\"\n",
    "                Evaluates model on a dev set\n",
    "                \"\"\"\n",
    "                feed_dict = {\n",
    "                  self.X: x_batch,\n",
    "                  self.Y: y_batch\n",
    "                }\n",
    "                step, summaries, loss, accuracy = sess.run(\n",
    "                    [global_step, dev_summary_op, self.loss, self.accuracy],\n",
    "                    feed_dict)\n",
    "                time_str = datetime.datetime.now().isoformat()\n",
    "                dev_summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/kwierman/Desktop/textrecog/notebook/runs/1516572312\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:08,  8.95s/it]\u001b[A\n",
      "2it [00:17,  8.50s/it]\u001b[A\n",
      "3it [00:24,  8.29s/it]\u001b[A\n",
      "4it [00:32,  8.11s/it]\u001b[A\n",
      "5it [00:39,  7.93s/it]\u001b[A\n",
      "6it [00:47,  7.95s/it]\u001b[A\n",
      "7it [00:56,  8.12s/it]\u001b[A\n",
      "8it [01:05,  8.15s/it]\u001b[A\n",
      "9it [01:12,  8.08s/it]\u001b[A\n",
      "10it [01:20,  8.05s/it]\u001b[A\n",
      "11it [01:28,  8.03s/it]\u001b[A\n",
      "12it [01:36,  8.00s/it]\u001b[A\n",
      "13it [01:43,  8.00s/it]\u001b[A\n",
      "14it [01:51,  7.98s/it]\u001b[A\n",
      "15it [01:59,  7.94s/it]\u001b[A\n",
      "16it [02:06,  7.89s/it]\u001b[A\n",
      "17it [02:13,  7.87s/it]\u001b[A\n",
      "18it [02:20,  7.79s/it]\u001b[A\n",
      "19it [02:25,  7.68s/it]\u001b[A\n",
      "20it [02:32,  7.64s/it]\u001b[A\n",
      "21it [02:40,  7.63s/it]\u001b[A\n",
      "22it [02:47,  7.62s/it]\u001b[A\n",
      "23it [02:55,  7.64s/it]\u001b[A\n",
      "24it [03:04,  7.68s/it]\u001b[A\n",
      "25it [03:12,  7.68s/it]\u001b[A\n",
      "26it [03:19,  7.68s/it]\u001b[A\n",
      "27it [03:27,  7.69s/it]\u001b[A\n",
      "28it [03:34,  7.68s/it]\u001b[A\n",
      "29it [03:42,  7.68s/it]\u001b[A\n",
      "30it [03:49,  7.67s/it]\u001b[A\n",
      "31it [03:58,  7.70s/it]\u001b[A\n",
      "32it [04:07,  7.74s/it]\u001b[A\n",
      "33it [04:18,  7.83s/it]\u001b[A\n",
      "34it [04:29,  7.94s/it]\u001b[A\n",
      "35it [04:39,  7.99s/it]\u001b[A\n",
      "36it [04:48,  8.01s/it]\u001b[A\n",
      "37it [04:56,  8.02s/it]\u001b[A\n",
      "38it [05:04,  8.02s/it]\u001b[A\n",
      "39it [05:14,  8.06s/it]\u001b[A\n",
      "40it [05:22,  8.05s/it]\u001b[A\n",
      "41it [05:30,  8.05s/it]\u001b[A\n",
      "42it [05:38,  8.07s/it]\u001b[A\n",
      "43it [05:47,  8.08s/it]\u001b[A\n",
      "44it [05:56,  8.09s/it]\u001b[A\n",
      "45it [06:04,  8.10s/it]\u001b[A\n",
      "46it [06:12,  8.10s/it]\u001b[A\n",
      "47it [06:21,  8.12s/it]\u001b[A\n",
      "48it [06:29,  8.12s/it]\u001b[A\n",
      "49it [06:37,  8.12s/it]\u001b[A\n",
      "50it [06:46,  8.12s/it]\u001b[A\n",
      "51it [06:54,  8.13s/it]\u001b[A\n",
      "52it [07:02,  8.13s/it]\u001b[A\n",
      "53it [07:11,  8.14s/it]\u001b[A\n",
      "54it [07:19,  8.14s/it]\u001b[A\n",
      "55it [07:28,  8.16s/it]\u001b[A\n",
      "56it [07:37,  8.17s/it]\u001b[A\n",
      "57it [07:46,  8.18s/it]\u001b[A\n",
      "58it [07:54,  8.19s/it]\u001b[A\n",
      "59it [08:02,  8.18s/it]\u001b[A\n",
      "60it [08:11,  8.19s/it]\u001b[A\n",
      "61it [08:18,  8.18s/it]\u001b[A\n",
      "62it [08:27,  8.18s/it]\u001b[A\n",
      "63it [08:36,  8.20s/it]\u001b[A\n",
      "64it [08:44,  8.20s/it]\u001b[A\n",
      "65it [08:52,  8.20s/it]\u001b[A\n",
      "66it [09:00,  8.20s/it]\u001b[A\n",
      "67it [09:09,  8.20s/it]\u001b[A\n",
      "68it [09:17,  8.20s/it]\u001b[A\n",
      "69it [09:26,  8.21s/it]\u001b[A\n",
      "70it [09:34,  8.21s/it]\u001b[A\n",
      "71it [09:42,  8.21s/it]\u001b[A\n",
      "72it [09:51,  8.21s/it]\u001b[A\n",
      "73it [09:59,  8.21s/it]\u001b[A\n",
      "74it [10:07,  8.21s/it]\u001b[A\n",
      "75it [10:15,  8.21s/it]\u001b[A\n",
      "76it [10:23,  8.21s/it]\u001b[A\n",
      "77it [10:31,  8.20s/it]\u001b[A\n",
      "78it [10:39,  8.20s/it]\u001b[A\n",
      "79it [10:47,  8.20s/it]\u001b[A\n",
      "80it [10:56,  8.20s/it]\u001b[A\n",
      "81it [11:04,  8.21s/it]\u001b[A\n",
      "82it [11:14,  8.22s/it]\u001b[A\n",
      "83it [11:22,  8.23s/it]\u001b[A\n",
      "84it [11:32,  8.24s/it]\u001b[A\n",
      "85it [11:41,  8.25s/it]\u001b[A\n",
      "86it [11:50,  8.26s/it]\u001b[A\n",
      "87it [11:59,  8.27s/it]\u001b[A\n",
      "88it [12:09,  8.29s/it]\u001b[A\n",
      "89it [12:20,  8.32s/it]\u001b[A\n",
      "90it [12:30,  8.34s/it]\u001b[A\n",
      "91it [12:38,  8.34s/it]\u001b[A\n",
      "92it [12:48,  8.35s/it]\u001b[A\n",
      "93it [12:59,  8.39s/it]\u001b[A\n",
      "94it [13:09,  8.40s/it]\u001b[A\n",
      "95it [13:17,  8.40s/it]\u001b[A\n",
      "96it [13:26,  8.40s/it]\u001b[A\n",
      "97it [13:34,  8.40s/it]\u001b[A\n",
      "98it [13:43,  8.40s/it]\u001b[A\n",
      "99it [13:52,  8.41s/it]\u001b[A\n",
      "100it [14:00,  8.41s/it]\u001b[A\n",
      "101it [14:08,  8.40s/it]\u001b[A\n",
      "102it [14:17,  8.40s/it]\u001b[A\n",
      "103it [14:25,  8.40s/it]\u001b[A\n",
      "104it [14:34,  8.41s/it]\u001b[A\n",
      "105it [14:42,  8.40s/it]\u001b[A\n",
      "106it [14:50,  8.40s/it]\u001b[A\n",
      "107it [14:59,  8.41s/it]\u001b[A\n",
      "108it [15:07,  8.40s/it]\u001b[A\n",
      "109it [15:15,  8.40s/it]\u001b[A\n",
      "110it [15:24,  8.40s/it]\u001b[A\n",
      "111it [15:32,  8.40s/it]\u001b[A\n",
      "112it [15:40,  8.40s/it]\u001b[A\n",
      "113it [15:49,  8.40s/it]\u001b[A\n",
      "114it [15:57,  8.40s/it]\u001b[A\n",
      "115it [16:06,  8.40s/it]\u001b[A\n",
      "116it [16:15,  8.41s/it]\u001b[A\n",
      "117it [16:24,  8.41s/it]\u001b[A\n",
      "118it [16:32,  8.41s/it]\u001b[A\n",
      "119it [16:40,  8.41s/it]\u001b[A\n",
      "120it [16:50,  8.42s/it]\u001b[A\n",
      "121it [16:58,  8.42s/it]\u001b[A\n",
      "122it [17:06,  8.42s/it]\u001b[A\n",
      "123it [17:15,  8.42s/it]\u001b[A\n",
      "124it [17:22,  8.41s/it]\u001b[A\n",
      "125it [17:31,  8.41s/it]\u001b[A\n",
      "126it [17:38,  8.40s/it]\u001b[A\n",
      "127it [17:46,  8.40s/it]\u001b[A\n",
      "128it [17:54,  8.39s/it]\u001b[A\n",
      "129it [18:02,  8.39s/it]\u001b[A\n",
      "130it [18:10,  8.39s/it]\u001b[A\n",
      "131it [18:18,  8.38s/it]\u001b[A\n",
      "132it [18:26,  8.38s/it]\u001b[A\n",
      "133it [18:34,  8.38s/it]\u001b[A\n",
      "134it [18:43,  8.39s/it]\u001b[A\n",
      "135it [18:51,  8.38s/it]\u001b[A\n",
      "136it [19:00,  8.38s/it]\u001b[A\n",
      "137it [19:07,  8.38s/it]\u001b[A\n",
      "138it [19:15,  8.38s/it]\u001b[A\n",
      "139it [19:23,  8.37s/it]\u001b[A\n",
      "140it [19:30,  8.36s/it]\u001b[A\n",
      "141it [19:38,  8.35s/it]\u001b[A\n",
      "142it [19:47,  8.37s/it]\u001b[A\n",
      "143it [19:57,  8.38s/it]\u001b[A\n",
      "144it [20:07,  8.39s/it]\u001b[A\n",
      "145it [20:16,  8.39s/it]\u001b[A\n",
      "146it [20:24,  8.39s/it]\u001b[A\n",
      "147it [20:33,  8.39s/it]\u001b[A\n",
      "148it [20:41,  8.39s/it]\u001b[A\n",
      "149it [20:49,  8.39s/it]\u001b[A\n",
      "150it [20:57,  8.39s/it]\u001b[A\n",
      "151it [21:06,  8.39s/it]\u001b[A\n",
      "152it [21:14,  8.38s/it]\u001b[A\n",
      "153it [21:23,  8.39s/it]\u001b[A\n",
      "154it [21:31,  8.39s/it]\u001b[A\n",
      "155it [21:40,  8.39s/it]\u001b[A\n",
      "156it [21:49,  8.40s/it]\u001b[A\n",
      "157it [21:59,  8.40s/it]\u001b[A\n",
      "158it [22:08,  8.41s/it]\u001b[A\n",
      "159it [22:17,  8.41s/it]\u001b[A\n",
      "160it [22:26,  8.42s/it]\u001b[A\n",
      "161it [22:35,  8.42s/it]\u001b[A\n",
      "162it [22:44,  8.42s/it]\u001b[A\n",
      "163it [22:52,  8.42s/it]\u001b[A\n",
      "164it [23:00,  8.42s/it]\u001b[A\n",
      "165it [23:09,  8.42s/it]\u001b[A\n",
      "166it [23:17,  8.42s/it]\u001b[A\n",
      "167it [23:25,  8.42s/it]\u001b[A\n",
      "168it [23:35,  8.42s/it]\u001b[A\n",
      "169it [23:44,  8.43s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    sess = tf.Session()\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN()\n",
    "        cnn.train(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
